{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anilesh05/Big_Data_Laboratory/blob/main/Find_grade_Using_Mapreduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Downloading and installing hadoop***"
      ],
      "metadata": {
        "id": "Zi9bK5z88-Pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Gp7HsNH56ldg",
        "outputId": "7fe64a4a-d2d7-4243-e8aa-77bc716c664c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-8-jdk is already the newest version (8u402-ga-2ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "--2024-04-03 14:27:55--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730107476 (696M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.6.tar.gz.1’\n",
            "\n",
            "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M  45.3MB/s    in 6.6s    \n",
            "\n",
            "2024-04-03 14:28:01 (106 MB/s) - ‘hadoop-3.3.6.tar.gz.1’ saved [730107476/730107476]\n",
            "\n",
            "ln: failed to create symbolic link '/usr/bin/container-executor': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hadoop': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hadoop.cmd': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hdfs': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hdfs.cmd': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/mapred': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/mapred.cmd': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/oom-listener': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/test-container-executor': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/yarn': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/yarn.cmd': File exists\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk\n",
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar fx hadoop-3.3.6.tar.gz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "!ln -s /content/hadoop-3.3.6/bin/* /usr/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Create Input Directory***"
      ],
      "metadata": {
        "id": "KY0k9xzzCp-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -mkdir input"
      ],
      "metadata": {
        "id": "mQtG3LtRBICN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Create mapper.py***"
      ],
      "metadata": {
        "id": "VZw1ck6fCgBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "# Define column names\n",
        "column_names = [\"Student ID\", \"Name\", \"Total Marks\", \"Grade\"]\n",
        "\n",
        "# Skip the header row\n",
        "header_skipped = False\n",
        "\n",
        "# Input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # Remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "\n",
        "    if not header_skipped:\n",
        "        # Skip the header row\n",
        "        header_skipped = True\n",
        "        continue\n",
        "\n",
        "    # Split the line into columns\n",
        "    columns = line.split(',')\n",
        "    if len(columns) == 3:\n",
        "        student_id, name, marks = columns\n",
        "        total_marks = sum(map(float, [marks]))\n",
        "        # Determine the grade\n",
        "        if total_marks >= 90:\n",
        "            grade = \"A+\"\n",
        "        elif total_marks >= 80:\n",
        "            grade = \"A\"\n",
        "        elif total_marks >= 70:\n",
        "            grade = \"B+\"\n",
        "        elif total_marks >= 60:\n",
        "            grade = \"B\"\n",
        "        elif total_marks >= 50:\n",
        "            grade = \"C\"\n",
        "        else:\n",
        "            grade = \"U\"\n",
        "        # Output the result\n",
        "        print(f\"{student_id},{name},{total_marks},{grade}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXvXT2u69FZs",
        "outputId": "db2b990c-b264-4108-e055-390c72ea41d2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Create Reducer.py***"
      ],
      "metadata": {
        "id": "rTrWutt0CmBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "# Input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # Remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # Split the line into columns\n",
        "    student_id, name, total_marks, grade = line.split(',')\n",
        "    # Output in table format\n",
        "    print(f\"{student_id:<5} {name:<15} {total_marks:<10} {grade}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHgpwstaA8J9",
        "outputId": "639cad85-1cda-48e6-87be-a8420e80132f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Write input file***"
      ],
      "metadata": {
        "id": "yxYjNtCmCtud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile input/marksheet.txt\n",
        "\n",
        "01,John,85\n",
        "02,Emma,92\n",
        "03,Michael,78\n",
        "04,Sophia,60\n",
        "05,William,88\n",
        "06,Isabella,75\n",
        "07,James,90\n",
        "08,Olivia,82\n",
        "09,Alexander,70\n",
        "10,Ava,95\n",
        "11,Ethan,72\n",
        "12,Mia,86\n",
        "13,Daniel,81\n",
        "14,Charlotte,79\n",
        "15,Joseph,94\n",
        "16,Abigail,87\n",
        "17,David,76\n",
        "18,Emily,93\n",
        "19,Benjamin,83\n",
        "20,Harper,77\n",
        "21,Matthew,91\n",
        "22,Elizabeth,84\n",
        "23,Jackson,34\n",
        "24,Ella,89\n",
        "25,Samuel,80\n",
        "26,Avery,96\n",
        "27,Henry,73\n",
        "28,Scarlett,97\n",
        "29,Gabriel,68\n",
        "30,Victoria,98\n",
        "31,Andrew,40\n",
        "32,Grace,99\n",
        "33,Ryan,65\n",
        "34,Chloe,100\n",
        "35,Lucas,66\n",
        "36,Sofia,98\n",
        "37,Christopher,67\n",
        "38,Liam,97\n",
        "39,Evelyn,64\n",
        "40,Dylan,96"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kuRzPvsBLvl",
        "outputId": "97270343-4432-41b6-d4f8-e8a71922ff0e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting input/marksheet.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Run hadoop mapreduce***"
      ],
      "metadata": {
        "id": "BAs7r8swCyB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar /content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "    -files mapper.py,reducer.py \\\n",
        "    -mapper mapper.py \\\n",
        "    -reducer reducer.py \\\n",
        "    -input input/marksheet.txt \\\n",
        "    -output output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td6jgpHGBYZ2",
        "outputId": "300872b3-04f4-4393-e950-a474a90493f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-03 14:46:32,014 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-03 14:46:32,124 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-03 14:46:32,124 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-03 14:46:32,144 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-03 14:46:32,465 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-03 14:46:32,487 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-03 14:46:32,685 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1277131040_0001\n",
            "2024-04-03 14:46:32,686 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-03 14:46:33,135 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1277131040_0001_f5b40699-43fc-4078-bf31-f8718b5b756f/mapper.py\n",
            "2024-04-03 14:46:33,152 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1277131040_0001_a3602169-9268-4aad-8b4c-1b613b053a8c/reducer.py\n",
            "2024-04-03 14:46:33,343 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-03 14:46:33,347 INFO mapreduce.Job: Running job: job_local1277131040_0001\n",
            "2024-04-03 14:46:33,354 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-03 14:46:33,356 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-03 14:46:33,368 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-03 14:46:33,368 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-03 14:46:33,529 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-03 14:46:33,534 INFO mapred.LocalJobRunner: Starting task: attempt_local1277131040_0001_m_000000_0\n",
            "2024-04-03 14:46:33,569 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-03 14:46:33,569 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-03 14:46:33,594 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-03 14:46:33,607 INFO mapred.MapTask: Processing split: file:/content/input/marksheet.txt:0+523\n",
            "2024-04-03 14:46:33,636 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-03 14:46:33,734 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-03 14:46:33,734 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-03 14:46:33,734 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-03 14:46:33,734 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-03 14:46:33,734 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-03 14:46:33,738 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-03 14:46:33,744 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2024-04-03 14:46:33,751 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-03 14:46:33,754 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-03 14:46:33,754 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-03 14:46:33,755 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-03 14:46:33,755 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-03 14:46:33,756 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-03 14:46:33,757 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-03 14:46:33,758 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-03 14:46:33,758 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-03 14:46:33,759 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-03 14:46:33,759 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-03 14:46:33,760 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-03 14:46:33,796 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-03 14:46:33,796 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-03 14:46:33,863 INFO streaming.PipeMapRed: Records R/W=41/1\n",
            "2024-04-03 14:46:33,867 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-03 14:46:33,868 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-03 14:46:33,872 INFO mapred.LocalJobRunner: \n",
            "2024-04-03 14:46:33,872 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-03 14:46:33,872 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-03 14:46:33,872 INFO mapred.MapTask: bufstart = 0; bufend = 744; bufvoid = 104857600\n",
            "2024-04-03 14:46:33,873 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214240(104856960); length = 157/6553600\n",
            "2024-04-03 14:46:33,882 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-03 14:46:33,896 INFO mapred.Task: Task:attempt_local1277131040_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-03 14:46:33,899 INFO mapred.LocalJobRunner: Records R/W=41/1\n",
            "2024-04-03 14:46:33,899 INFO mapred.Task: Task 'attempt_local1277131040_0001_m_000000_0' done.\n",
            "2024-04-03 14:46:33,913 INFO mapred.Task: Final Counters for attempt_local1277131040_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=143314\n",
            "\t\tFILE: Number of bytes written=787944\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=41\n",
            "\t\tMap output records=40\n",
            "\t\tMap output bytes=744\n",
            "\t\tMap output materialized bytes=830\n",
            "\t\tInput split bytes=85\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=40\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=306184192\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=523\n",
            "2024-04-03 14:46:33,913 INFO mapred.LocalJobRunner: Finishing task: attempt_local1277131040_0001_m_000000_0\n",
            "2024-04-03 14:46:33,914 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-03 14:46:33,920 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-03 14:46:33,920 INFO mapred.LocalJobRunner: Starting task: attempt_local1277131040_0001_r_000000_0\n",
            "2024-04-03 14:46:33,931 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-03 14:46:33,931 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-03 14:46:33,932 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-03 14:46:33,936 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7744f899\n",
            "2024-04-03 14:46:33,939 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-03 14:46:33,972 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-03 14:46:33,987 INFO reduce.EventFetcher: attempt_local1277131040_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-03 14:46:34,033 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1277131040_0001_m_000000_0 decomp: 826 len: 830 to MEMORY\n",
            "2024-04-03 14:46:34,038 INFO reduce.InMemoryMapOutput: Read 826 bytes from map-output for attempt_local1277131040_0001_m_000000_0\n",
            "2024-04-03 14:46:34,041 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 826, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->826\n",
            "2024-04-03 14:46:34,042 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-03 14:46:34,043 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-03 14:46:34,044 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-03 14:46:34,057 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-03 14:46:34,057 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 809 bytes\n",
            "2024-04-03 14:46:34,062 INFO reduce.MergeManagerImpl: Merged 1 segments, 826 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-03 14:46:34,063 INFO reduce.MergeManagerImpl: Merging 1 files, 830 bytes from disk\n",
            "2024-04-03 14:46:34,064 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-03 14:46:34,064 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-03 14:46:34,065 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 809 bytes\n",
            "2024-04-03 14:46:34,066 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-03 14:46:34,072 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2024-04-03 14:46:34,076 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-03 14:46:34,078 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-03 14:46:34,110 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-03 14:46:34,111 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-03 14:46:34,185 INFO streaming.PipeMapRed: Records R/W=40/1\n",
            "2024-04-03 14:46:34,190 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-03 14:46:34,191 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-03 14:46:34,192 INFO mapred.Task: Task:attempt_local1277131040_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-03 14:46:34,193 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-03 14:46:34,193 INFO mapred.Task: Task attempt_local1277131040_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-03 14:46:34,195 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1277131040_0001_r_000000_0' to file:/content/output\n",
            "2024-04-03 14:46:34,196 INFO mapred.LocalJobRunner: Records R/W=40/1 > reduce\n",
            "2024-04-03 14:46:34,197 INFO mapred.Task: Task 'attempt_local1277131040_0001_r_000000_0' done.\n",
            "2024-04-03 14:46:34,197 INFO mapred.Task: Final Counters for attempt_local1277131040_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=145006\n",
            "\t\tFILE: Number of bytes written=790256\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=40\n",
            "\t\tReduce shuffle bytes=830\n",
            "\t\tReduce input records=40\n",
            "\t\tReduce output records=40\n",
            "\t\tSpilled Records=40\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=306184192\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1482\n",
            "2024-04-03 14:46:34,197 INFO mapred.LocalJobRunner: Finishing task: attempt_local1277131040_0001_r_000000_0\n",
            "2024-04-03 14:46:34,198 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-03 14:46:34,365 INFO mapreduce.Job: Job job_local1277131040_0001 running in uber mode : false\n",
            "2024-04-03 14:46:34,366 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-03 14:46:34,368 INFO mapreduce.Job: Job job_local1277131040_0001 completed successfully\n",
            "2024-04-03 14:46:34,377 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=288320\n",
            "\t\tFILE: Number of bytes written=1578200\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=41\n",
            "\t\tMap output records=40\n",
            "\t\tMap output bytes=744\n",
            "\t\tMap output materialized bytes=830\n",
            "\t\tInput split bytes=85\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=40\n",
            "\t\tReduce shuffle bytes=830\n",
            "\t\tReduce input records=40\n",
            "\t\tReduce output records=40\n",
            "\t\tSpilled Records=80\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=612368384\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=523\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1482\n",
            "2024-04-03 14:46:34,377 INFO streaming.StreamJob: Output directory: output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Display Output***"
      ],
      "metadata": {
        "id": "DWOni8kIC4pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go_FpoLFBfV-",
        "outputId": "666898df-e31c-4fe9-b261-0e890390c016"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01    John            85.0       A\t\n",
            "02    Emma            92.0       A+\t\n",
            "03    Michael         78.0       B+\t\n",
            "04    Sophia          60.0       B\t\n",
            "05    William         88.0       A\t\n",
            "06    Isabella        75.0       B+\t\n",
            "07    James           90.0       A+\t\n",
            "08    Olivia          82.0       A\t\n",
            "09    Alexander       70.0       B+\t\n",
            "10    Ava             95.0       A+\t\n",
            "11    Ethan           72.0       B+\t\n",
            "12    Mia             86.0       A\t\n",
            "13    Daniel          81.0       A\t\n",
            "14    Charlotte       79.0       B+\t\n",
            "15    Joseph          94.0       A+\t\n",
            "16    Abigail         87.0       A\t\n",
            "17    David           76.0       B+\t\n",
            "18    Emily           93.0       A+\t\n",
            "19    Benjamin        83.0       A\t\n",
            "20    Harper          77.0       B+\t\n",
            "21    Matthew         91.0       A+\t\n",
            "22    Elizabeth       84.0       A\t\n",
            "23    Jackson         34.0       U\t\n",
            "24    Ella            89.0       A\t\n",
            "25    Samuel          80.0       A\t\n",
            "26    Avery           96.0       A+\t\n",
            "27    Henry           73.0       B+\t\n",
            "28    Scarlett        97.0       A+\t\n",
            "29    Gabriel         68.0       B\t\n",
            "30    Victoria        98.0       A+\t\n",
            "31    Andrew          40.0       U\t\n",
            "32    Grace           99.0       A+\t\n",
            "33    Ryan            65.0       B\t\n",
            "34    Chloe           100.0      A+\t\n",
            "35    Lucas           66.0       B\t\n",
            "36    Sofia           98.0       A+\t\n",
            "37    Christopher     67.0       B\t\n",
            "38    Liam            97.0       A+\t\n",
            "39    Evelyn          64.0       B\t\n",
            "40    Dylan           96.0       A+\t\n"
          ]
        }
      ]
    }
  ]
}
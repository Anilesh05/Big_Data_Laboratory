{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anilesh05/Big_Data_Laboratory/blob/main/10_maximum_electrical_consumption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and install hadoop"
      ],
      "metadata": {
        "id": "BPJXWgx7hu12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UR7yMNueALI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9339c4e7-de7b-4a4b-db0a-09026a39f097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-8-jdk is already the newest version (8u402-ga-2ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "--2024-04-14 17:29:23--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730107476 (696M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.6.tar.gz.1’\n",
            "\n",
            "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M   221MB/s    in 3.2s    \n",
            "\n",
            "2024-04-14 17:29:26 (221 MB/s) - ‘hadoop-3.3.6.tar.gz.1’ saved [730107476/730107476]\n",
            "\n",
            "ln: failed to create symbolic link '/usr/bin/container-executor': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hadoop': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hadoop.cmd': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hdfs': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/hdfs.cmd': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/mapred': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/mapred.cmd': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/oom-listener': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/test-container-executor': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/yarn': File exists\n",
            "ln: failed to create symbolic link '/usr/bin/yarn.cmd': File exists\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk\n",
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar fx hadoop-3.3.6.tar.gz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "!ln -s /content/hadoop-3.3.6/bin/* /usr/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create mapper.py"
      ],
      "metadata": {
        "id": "D86e6Fyihzn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # Split the line into date and temperature\n",
        "    parts = line.strip().split(\",\")\n",
        "    if len(parts) == 2:\n",
        "        date, temperature = parts\n",
        "        year = date[:4]\n",
        "        print(f\"{year}\\t{temperature}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzPsPSMZh2x6",
        "outputId": "b7fb51c2-4647-4758-ff43-a7b864331649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create reducer.py"
      ],
      "metadata": {
        "id": "sIPXLPT8iUZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "current_year = None\n",
        "max_consumption = -float('inf')\n",
        "\n",
        "print(\"{:<10} {:<15}\".format(\"Year\", \"Consumption\"))\n",
        "print()\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    year, consumption = line.split(\"\\t\")\n",
        "    consumption = float(consumption)\n",
        "\n",
        "    if current_year == year:\n",
        "        max_consumption = max(max_consumption, consumption)\n",
        "    else:\n",
        "        if current_year is not None:\n",
        "            print(\"{:<10} {:<15}\".format(current_year, max_consumption))\n",
        "        current_year = year\n",
        "        max_consumption = consumption\n",
        "\n",
        "if current_year is not None:\n",
        "    print(\"{:<10} {:<15}\".format(current_year, max_consumption))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA4rjNS5iXXf",
        "outputId": "67be2c6f-e230-496b-ec99-48ab6fb4f3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a input directory"
      ],
      "metadata": {
        "id": "4sRNKOpdieYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir input"
      ],
      "metadata": {
        "id": "80AXrKD5idvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile input/input.txt\n",
        "2010-01-01,150\n",
        "2010-01-02,160\n",
        "2010-01-03,170\n",
        "2010-01-04,180\n",
        "2010-01-05,190\n",
        "2010-01-06,200\n",
        "2010-01-07,210\n",
        "2010-01-08,220\n",
        "2010-01-09,230\n",
        "2010-01-10,240\n",
        "2011-01-01,155\n",
        "2011-01-02,165\n",
        "2011-01-03,175\n",
        "2011-01-04,185\n",
        "2011-01-05,195\n",
        "2011-01-06,205\n",
        "2011-01-07,215\n",
        "2011-01-08,225\n",
        "2011-01-09,235\n",
        "2011-01-10,245\n",
        "2012-01-01,160\n",
        "2012-01-02,170\n",
        "2012-01-03,180\n",
        "2012-01-04,190\n",
        "2012-01-05,200\n",
        "2012-01-06,210\n",
        "2012-01-07,220\n",
        "2012-01-08,230\n",
        "2012-01-09,240\n",
        "2012-01-10,250\n",
        "2013-01-01,165\n",
        "2013-01-02,175\n",
        "2013-01-03,185\n",
        "2013-01-04,195\n",
        "2013-01-05,205\n",
        "2013-01-06,215\n",
        "2013-01-07,225\n",
        "2013-01-08,235\n",
        "2013-01-09,245\n",
        "2013-01-10,255\n",
        "2014-01-01,170\n",
        "2014-01-02,180\n",
        "2014-01-03,190\n",
        "2014-01-04,200\n",
        "2014-01-05,210\n",
        "2014-01-06,220\n",
        "2014-01-07,230\n",
        "2014-01-08,240\n",
        "2014-01-09,250\n",
        "2014-01-10,260\n",
        "2015-01-01,175\n",
        "2015-01-02,185\n",
        "2015-01-03,195\n",
        "2015-01-04,205\n",
        "2015-01-05,215\n",
        "2015-01-06,225\n",
        "2015-01-07,235\n",
        "2015-01-08,245\n",
        "2015-01-09,255\n",
        "2015-01-10,265\n",
        "2016-01-01,180\n",
        "2016-01-02,190\n",
        "2016-01-03,200\n",
        "2016-01-04,210\n",
        "2016-01-05,220\n",
        "2016-01-06,230\n",
        "2016-01-07,240\n",
        "2016-01-08,250\n",
        "2016-01-09,260\n",
        "2016-01-10,270\n",
        "2017-01-01,185\n",
        "2017-01-02,195\n",
        "2017-01-03,205\n",
        "2017-01-04,215\n",
        "2017-01-05,225\n",
        "2017-01-06,235\n",
        "2017-01-07,245\n",
        "2017-01-08,255\n",
        "2017-01-09,265\n",
        "2017-01-10,275\n",
        "2018-01-01,190\n",
        "2018-01-02,200\n",
        "2018-01-03,210\n",
        "2018-01-04,220\n",
        "2018-01-05,230\n",
        "2018-01-06,240\n",
        "2018-01-07,250\n",
        "2018-01-08,260\n",
        "2018-01-09,270\n",
        "2018-01-10,280\n",
        "2019-01-01,195\n",
        "2019-01-02,205\n",
        "2019-01-03,215\n",
        "2019-01-04,225\n",
        "2019-01-05,235\n",
        "2019-01-06,245\n",
        "2019-01-07,255\n",
        "2019-01-08,265\n",
        "2019-01-09,275\n",
        "2019-01-10,285\n",
        "2020-01-01,200\n",
        "2020-01-02,210\n",
        "2020-01-03,220\n",
        "2020-01-04,230\n",
        "2020-01-05,240\n",
        "2020-01-06,250\n",
        "2020-01-07,260\n",
        "2020-01-08,270\n",
        "2020-01-09,280\n",
        "2020-01-10,290\n",
        "2021-01-01,205\n",
        "2021-01-02,215\n",
        "2021-01-03,225\n",
        "2021-01-04,235\n",
        "2021-01-05,245\n",
        "2021-01-06,255\n",
        "2021-01-07,265\n",
        "2021-01-08,275\n",
        "2021-01-09,285\n",
        "2021-01-10,295\n",
        "2022-01-01,210\n",
        "2022-01-02,220\n",
        "2022-01-03,230\n",
        "2022-01-04,240\n",
        "2022-01-05,250\n",
        "2022-01-06,260\n",
        "2022-01-07,270\n",
        "2022-01-08,280\n",
        "2022-01-09,290\n",
        "2022-01-10,300\n",
        "2023-01-01,215\n",
        "2023-01-02,225\n",
        "2023-01-03,235\n",
        "2023-01-04,245\n",
        "2023-01-05,255\n",
        "2023-01-06,265\n",
        "2023-01-07,275\n",
        "2023-01-08,285\n",
        "2023-01-09,295\n",
        "2023-01-10,305"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IuzcKMOiaMV",
        "outputId": "111a9591-0a1e-4194-81f7-2bf2e9ad7def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing input/input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar /content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "    -files mapper.py,reducer.py \\\n",
        "    -mapper mapper.py \\\n",
        "    -reducer reducer.py \\\n",
        "    -input input \\\n",
        "    -output output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfzQEPUNi6KI",
        "outputId": "392fbcc6-5402-49fe-cfdf-f82b8500406b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-14 17:29:47,764 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-14 17:29:47,948 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-14 17:29:47,948 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-14 17:29:47,976 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-14 17:29:48,691 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-14 17:29:48,721 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-14 17:29:48,989 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1031904103_0001\n",
            "2024-04-14 17:29:48,989 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-14 17:29:49,424 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1031904103_0001_567c4bfb-47e8-429e-aa26-e8dbdfc92047/mapper.py\n",
            "2024-04-14 17:29:49,446 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1031904103_0001_294b451f-cf55-4c51-9f59-3e715be53c91/reducer.py\n",
            "2024-04-14 17:29:49,646 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-14 17:29:49,648 INFO mapreduce.Job: Running job: job_local1031904103_0001\n",
            "2024-04-14 17:29:49,655 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-14 17:29:49,752 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-14 17:29:49,758 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-14 17:29:49,758 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-14 17:29:49,817 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-14 17:29:49,822 INFO mapred.LocalJobRunner: Starting task: attempt_local1031904103_0001_m_000000_0\n",
            "2024-04-14 17:29:49,859 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-14 17:29:49,864 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-14 17:29:49,890 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-14 17:29:49,900 INFO mapred.MapTask: Processing split: file:/content/input/input.txt:0+2100\n",
            "2024-04-14 17:29:49,914 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-14 17:29:50,007 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-14 17:29:50,007 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-14 17:29:50,007 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-14 17:29:50,007 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-14 17:29:50,007 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-14 17:29:50,015 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-14 17:29:50,027 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2024-04-14 17:29:50,034 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-14 17:29:50,036 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-14 17:29:50,037 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-14 17:29:50,037 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-14 17:29:50,037 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-14 17:29:50,038 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-14 17:29:50,039 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-14 17:29:50,040 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-14 17:29:50,040 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-14 17:29:50,040 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-14 17:29:50,041 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-14 17:29:50,041 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-14 17:29:50,063 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-14 17:29:50,064 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-14 17:29:50,067 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-14 17:29:50,141 INFO streaming.PipeMapRed: Records R/W=140/1\n",
            "2024-04-14 17:29:50,153 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-14 17:29:50,154 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-14 17:29:50,158 INFO mapred.LocalJobRunner: \n",
            "2024-04-14 17:29:50,159 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-14 17:29:50,159 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-14 17:29:50,159 INFO mapred.MapTask: bufstart = 0; bufend = 1260; bufvoid = 104857600\n",
            "2024-04-14 17:29:50,159 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213840(104855360); length = 557/6553600\n",
            "2024-04-14 17:29:50,179 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-14 17:29:50,195 INFO mapred.Task: Task:attempt_local1031904103_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-14 17:29:50,200 INFO mapred.LocalJobRunner: Records R/W=140/1\n",
            "2024-04-14 17:29:50,200 INFO mapred.Task: Task 'attempt_local1031904103_0001_m_000000_0' done.\n",
            "2024-04-14 17:29:50,210 INFO mapred.Task: Final Counters for attempt_local1031904103_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=144411\n",
            "\t\tFILE: Number of bytes written=788146\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=140\n",
            "\t\tMap output records=140\n",
            "\t\tMap output bytes=1260\n",
            "\t\tMap output materialized bytes=1546\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=140\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=311951360\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2100\n",
            "2024-04-14 17:29:50,211 INFO mapred.LocalJobRunner: Finishing task: attempt_local1031904103_0001_m_000000_0\n",
            "2024-04-14 17:29:50,211 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-14 17:29:50,215 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-14 17:29:50,215 INFO mapred.LocalJobRunner: Starting task: attempt_local1031904103_0001_r_000000_0\n",
            "2024-04-14 17:29:50,225 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-14 17:29:50,225 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-14 17:29:50,226 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-14 17:29:50,230 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@53664f87\n",
            "2024-04-14 17:29:50,231 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-14 17:29:50,251 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-14 17:29:50,255 INFO reduce.EventFetcher: attempt_local1031904103_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-14 17:29:50,304 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1031904103_0001_m_000000_0 decomp: 1542 len: 1546 to MEMORY\n",
            "2024-04-14 17:29:50,313 INFO reduce.InMemoryMapOutput: Read 1542 bytes from map-output for attempt_local1031904103_0001_m_000000_0\n",
            "2024-04-14 17:29:50,319 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1542, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1542\n",
            "2024-04-14 17:29:50,322 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-14 17:29:50,323 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-14 17:29:50,323 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-14 17:29:50,332 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-14 17:29:50,332 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1535 bytes\n",
            "2024-04-14 17:29:50,337 INFO reduce.MergeManagerImpl: Merged 1 segments, 1542 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-14 17:29:50,339 INFO reduce.MergeManagerImpl: Merging 1 files, 1546 bytes from disk\n",
            "2024-04-14 17:29:50,340 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-14 17:29:50,340 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-14 17:29:50,341 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1535 bytes\n",
            "2024-04-14 17:29:50,342 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-14 17:29:50,346 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2024-04-14 17:29:50,350 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-14 17:29:50,352 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-14 17:29:50,385 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-14 17:29:50,385 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-14 17:29:50,390 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-14 17:29:50,468 INFO streaming.PipeMapRed: Records R/W=140/1\n",
            "2024-04-14 17:29:50,474 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-14 17:29:50,475 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-14 17:29:50,476 INFO mapred.Task: Task:attempt_local1031904103_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-14 17:29:50,477 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-14 17:29:50,477 INFO mapred.Task: Task attempt_local1031904103_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-14 17:29:50,480 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1031904103_0001_r_000000_0' to file:/content/output\n",
            "2024-04-14 17:29:50,480 INFO mapred.LocalJobRunner: Records R/W=140/1 > reduce\n",
            "2024-04-14 17:29:50,481 INFO mapred.Task: Task 'attempt_local1031904103_0001_r_000000_0' done.\n",
            "2024-04-14 17:29:50,481 INFO mapred.Task: Final Counters for attempt_local1031904103_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=147535\n",
            "\t\tFILE: Number of bytes written=790126\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=14\n",
            "\t\tReduce shuffle bytes=1546\n",
            "\t\tReduce input records=140\n",
            "\t\tReduce output records=16\n",
            "\t\tSpilled Records=140\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=311951360\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=434\n",
            "2024-04-14 17:29:50,481 INFO mapred.LocalJobRunner: Finishing task: attempt_local1031904103_0001_r_000000_0\n",
            "2024-04-14 17:29:50,481 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-14 17:29:50,749 INFO mapreduce.Job: Job job_local1031904103_0001 running in uber mode : false\n",
            "2024-04-14 17:29:50,750 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-14 17:29:50,751 INFO mapreduce.Job: Job job_local1031904103_0001 completed successfully\n",
            "2024-04-14 17:29:50,761 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=291946\n",
            "\t\tFILE: Number of bytes written=1578272\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=140\n",
            "\t\tMap output records=140\n",
            "\t\tMap output bytes=1260\n",
            "\t\tMap output materialized bytes=1546\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=14\n",
            "\t\tReduce shuffle bytes=1546\n",
            "\t\tReduce input records=140\n",
            "\t\tReduce output records=16\n",
            "\t\tSpilled Records=280\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=623902720\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2100\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=434\n",
            "2024-04-14 17:29:50,762 INFO streaming.StreamJob: Output directory: output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzilWT1_i-Og",
        "outputId": "7b3ac31d-658c-4683-e9ce-28cb7c47bdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year       Consumption    \t\n",
            "\t\n",
            "2010       240.0          \t\n",
            "2011       245.0          \t\n",
            "2012       250.0          \t\n",
            "2013       255.0          \t\n",
            "2014       260.0          \t\n",
            "2015       265.0          \t\n",
            "2016       270.0          \t\n",
            "2017       275.0          \t\n",
            "2018       280.0          \t\n",
            "2019       285.0          \t\n",
            "2020       290.0          \t\n",
            "2021       295.0          \t\n",
            "2022       300.0          \t\n",
            "2023       305.0          \t\n"
          ]
        }
      ]
    }
  ]
}